{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125e507c",
   "metadata": {},
   "source": [
    "<center>\n",
    "        <div><b>Assignment of</b><h2>Natural language Processing</h2>\n",
    "        |||<br></div>\n",
    "        <div>Submitted By:<br>\n",
    "        <b>Rusan Vaidya, 161436</b></div><br> \n",
    "        <div>Sumitted To:<br>\n",
    "        <b>Mr.Roshan Koju</b></div><br>\n",
    "        Department of IT Engineering<br> \n",
    "        <b><u>NEPAL COLLEGE OF INFORMATION TECHNOLOGY</u></b><br>\n",
    "        Balkumari, Lalitpur, Nepal.\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3544374",
   "metadata": {},
   "source": [
    "# Question\n",
    "Discuss what factors should you consider while designing and implementing software to detect plagiarism and para phasing? Develop sample application* for your arguments and prepare a report. * note: Application is not necessarily your own but you need to explain methodology properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24658154",
   "metadata": {},
   "source": [
    "# Palgarism\n",
    "<p>Palgarism is the process where the original content, work or idea presented by someone is represented as their own.\n",
    "Let us see how the palgarism check works through a simple program given below</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc9d45d",
   "metadata": {},
   "source": [
    "# Factors that is to be considered for checking palgarism are as follows:\n",
    "\n",
    "<li>Removal of stopwords which is most frequently occuring word which is to be ignored</li>\n",
    "<li>Changing the text in the form of vector.</li>\n",
    "<li>Converting the vector into an array form</li>\n",
    "<li>Applying Cosine Similarity to identify the similarities of the text</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a0da59",
   "metadata": {},
   "source": [
    "<b>HERE LET'S READ THE ORIGINAL CONTENT</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f515b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "# o_text = [open(\"example.docx\", encoding='utf-8').read()][0]\n",
    "def read_docx(file_path):\n",
    "    doc = docx.Document(file_path)\n",
    "    text = []\n",
    "    for para in doc.paragraphs:\n",
    "        text.append(para.text)\n",
    "    return \" \".join(text)\n",
    "\n",
    "o_text = read_docx(\"example.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad99751",
   "metadata": {},
   "source": [
    "<b>NOW CREATING VECTORIZATION USING scikit-learn i.e sklearn module by and importing TfidfVectorizer</b>\n",
    "\n",
    "TfidfVectorizer converts into vector.\n",
    "This is another method which is based on the frequency method but it is different to the bag-of-words approach in the sense that it takes into account, not just the occurrence of a word in a single document (or tweet) but in the entire corpus.\n",
    "\n",
    "TF-IDF works by penalizing the common words by assigning them lower weights while giving importance to words which are rare in the entire corpus but appear in good numbers in few documents.\n",
    "\n",
    "Letâ€™s have a look at the important terms related to TF-IDF:\n",
    "\n",
    "    TF = (Number of times term t appears in a document)/(Number of terms in the document)\n",
    "    IDF = log(N/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n",
    "    TF-IDF = TF*IDF\n",
    "\n",
    "\n",
    "So let's create define the vectorization function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "29abfbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def vectorization(text1, text2):\n",
    "    vector = TfidfVectorizer(stop_words='english')\n",
    "    v = vector.fit_transform([text1, text2])\n",
    "    x = v[0].toarray().flatten()\n",
    "    y = v[1].toarray().flatten()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a16f911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def preprocess(text):\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24df964",
   "metadata": {},
   "source": [
    "### Here the above function \n",
    "<h3><li>first tokenize word and turn it into the vector form using TfidVectorizer()</li>\n",
    "<li>then it is trained using fit_transform() and </li>\n",
    "<li>then converted into array using toarray() </li></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b306afe",
   "metadata": {},
   "source": [
    "<b>NOW TO CHECK SIMILARITY BETWEEN THE TWO VECTORS WE USE COSINE SIMILARITY</b>\n",
    "<p>\n",
    "The cosine similarity is used to compare the the two vectors. It defines the similarity of the vectors.<br>\n",
    "Let the vectors be u and v then,<br>\n",
    "cosine similarity is given by <br>\n",
    "\n",
    "<h1>similarity(u,v) = <sup>(u*v.T)</sup>&frasl;<sub>/(||u||<sub>2</sub>*||v||<sub>2</sub>)</sub></h1>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d610fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cosine_similarity(u, v):\n",
    "    dot = np.dot(u,v)\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    cosine_similarity = dot/(norm_u*norm_v)    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "134cf13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_text = [open('test.txt', encoding='utf-8').read()][0]\n",
    "new_text = read_docx(\"test.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fdea7907",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt1 = preprocess(o_text)\n",
    "txt2 = preprocess(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e6b25a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "vt1, vt2 = vectorization(txt1, txt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ba851f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The palgarism check shows the 2 text is 57.91% similar.\n"
     ]
    }
   ],
   "source": [
    "result = cosine_similarity(vt1,vt2)\n",
    "if result > 0.5:\n",
    "    print(\"The palgarism check shows the 2 text is {:.2f}% similar.\".format(float(result*100)))\n",
    "else:\n",
    "    print(\"No plagiarism detected with similarity score: {:.2f}% \".format(float(result*100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f104a00",
   "metadata": {},
   "source": [
    "# Paraphasing\n",
    "Paraphrase is also a communication tool that serves, among other things, to confirm the level of understanding of what has been said. Rephrasing is most often made in the form of longer speeches, therefore only general sense and utterance are given away."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1a64ef",
   "metadata": {},
   "source": [
    "# Factors that is to be considered for checking paraphsing are as follows:\n",
    "<li>word_tokenize is used to tokenize each word or break down of word from scentences.</li>\n",
    "<li>pos_tag is used to identify the grammatical group of a given word</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75738c0e",
   "metadata": {},
   "source": [
    "<b>Word Tokenize is done to break down a scentence into words and create a list of the words</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d487f786",
   "metadata": {},
   "source": [
    "<b>Whether it is a NOUN, PRONOUN, ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c17241",
   "metadata": {},
   "source": [
    "<li>Here we define a function tag where the sentence is tokenized and creates a list of words from the sentence.</li>\n",
    "<li>Then, the post_tag takes words and looks for the relationship between the words</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "69e95eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def tag(sentence):\n",
    "    words = word_tokenize(sentence)\n",
    "    words = pos_tag(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8fc56bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tag(\"Hello World, welcome to python programming I am R.Vaidya\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e512d5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('World', 'NNP'),\n",
       " (',', ','),\n",
       " ('welcome', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('python', 'VB'),\n",
       " ('programming', 'VBG'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('R.Vaidya', 'JJ')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92525f6b",
   "metadata": {},
   "source": [
    "### Here the paraphrase function checks the a set of words are noun NN, verb VB, adjective JJ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eebab122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase(tag):\n",
    "    return tag.startswith('NN') or tag == 'VB' or tag.startswith('JJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "56c7ab32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase(z[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ffa37",
   "metadata": {},
   "source": [
    "### Here the pos function checks if the word is noun or a verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4c8ff894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8885a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos(z[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64efa93",
   "metadata": {},
   "source": [
    "### Now let's define a function called synonyms which takes word and tag as argument then checks similar words in the corpus of from the library nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "336a521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonyms(word, tag):\n",
    "    lemma_lists = [word12.lemmas() for word12 in wn.synsets(word, pos(tag))]\n",
    "    similar_words = [lemma.name() for lemma in sum(lemma_lists, [])]\n",
    "    return set(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2ea53ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ['hello', 'howdy', 'how-do-you-do', 'hullo', 'hi']]\n",
      "Hello []\n",
      "['World', ['reality', 'humans', 'world', 'worldly_concern', 'existence', 'humanity', 'macrocosm', 'humankind', 'domain', 'human_beings', 'man', 'cosmos', 'earthly_concern', 'universe', 'mankind', 'public', 'globe', 'creation', 'Earth', 'earth', 'populace', 'human_race']]\n",
      "World []\n",
      ", []\n",
      "welcome []\n",
      "to []\n",
      "python []\n",
      "programming []\n",
      "I []\n",
      "am []\n",
      "R.Vaidya []\n"
     ]
    }
   ],
   "source": [
    "for (word, tagg) in z:\n",
    "    if paraphrase(tagg):\n",
    "        syns = synonyms(word, tagg)\n",
    "        if syns:\n",
    "            if len(syns) > 1:\n",
    "                print([word, list(syns)])\n",
    "    print(word,[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da77134",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "From this assignment we have clear idea of how palgarism and paraphasing works in the natural laguage processing. I would like to thank Mr.Roshan Koju Sir for guiding and providing knowlege about nltk, bert, spacy, tensorflow, pytorch and many more python libraries and module to develop a natural language processing program."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb0c0c7f69cb158611da02291b918a45a005617ca1ccf9e5032e2156efadf0fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
